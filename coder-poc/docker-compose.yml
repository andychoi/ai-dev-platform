# Coder WebIDE PoC - Docker Compose Configuration
# This setup provides a complete local Coder environment for testing

services:
  # PostgreSQL database (shared by Coder, Authentik, Platform)
  postgres:
    image: postgres:17-alpine
    container_name: postgres
    restart: unless-stopped
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-postgres}
      POSTGRES_DB: postgres
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./postgres/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - coder-network

  # Coder control plane
  coder:
    image: ghcr.io/coder/coder:latest
    container_name: coder-server
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      authentik-server:
        condition: service_healthy
    environment:
      # Database connection
      CODER_PG_CONNECTION_URL: "postgresql://coder:coder@postgres:5432/coder?sslmode=disable"

      # Access URLs
      # Use host.docker.internal so workspace containers can reach Coder server
      CODER_ACCESS_URL: "${CODER_ACCESS_URL:-https://host.docker.internal:7443}"
      CODER_HTTP_ADDRESS: "0.0.0.0:7080"

      # TLS Configuration (required for browser secure context / crypto.subtle)
      # Without HTTPS, extension webviews (Roo Code, etc.) render blank
      CODER_TLS_ENABLE: "true"
      CODER_TLS_ADDRESS: "0.0.0.0:7443"
      CODER_TLS_CERT_FILE: "/certs/coder.crt"
      CODER_TLS_KEY_FILE: "/certs/coder.key"
      CODER_TLS_MIN_VERSION: "tls12"
      # CODER_REDIRECT_TO_ACCESS_URL: "true"  # Disabled: keeps HTTP API working for scripts

      # Security settings
      # Secure cookies enabled since we now use HTTPS
      CODER_SECURE_AUTH_COOKIE: "${CODER_SECURE_AUTH_COOKIE:-true}"
      CODER_TELEMETRY: "false"

      # Provisioner settings
      CODER_PROVISIONER_DAEMONS: "3"

      # Session settings - reduced for security
      CODER_MAX_SESSION_EXPIRY: "${CODER_MAX_SESSION_EXPIRY:-8h}"

      # Rate limiting - enabled by default, can disable for local testing only
      # SECURITY: Do NOT set CODER_RATE_LIMIT_DISABLE_ALL in production
      CODER_RATE_LIMIT_API: "${CODER_RATE_LIMIT_API:-512}"
      CODER_RATE_LIMIT_DISABLE_ALL: "${CODER_RATE_LIMIT_DISABLE_ALL:-false}"

      # Security: Workspace access controls
      # Prevent admins from accessing other users' workspaces
      # Owner can still manage workspaces (start/stop/delete) but cannot open terminals or IDE
      CODER_DISABLE_OWNER_WORKSPACE_ACCESS: "true"

      # Disable workspace sharing between users
      CODER_DISABLE_WORKSPACE_SHARING: "true"

      # Path-based apps (required when not using wildcard DNS for subdomain routing)
      # Set to "true" in production with proper subdomain setup
      CODER_DISABLE_PATH_APPS: "false"

      # Logging
      CODER_VERBOSE: "false"
      CODER_LOG_HUMAN: "/dev/stderr"

      # AI Bridge - DISABLED
      # Coder's built-in AI chat is disabled because we use LiteLLM + Roo Code
      # inside workspaces instead. This prevents the "Sign in to GitHub" popup
      # and removes the AI chat sidebar from the Coder dashboard.
      CODER_AIBRIDGE_ENABLED: "false"
      CODER_HIDE_AI_TASKS: "true"

      # OIDC Configuration (Authentik SSO)
      # IMPORTANT: Must use host.docker.internal (NOT authentik-server) because Coder
      # redirects the browser to the authorization_endpoint from the discovery document.
      # Authentik echoes back the hostname it was accessed from, so if Coder fetches
      # discovery via "authentik-server", the browser gets redirected to an unresolvable hostname.
      CODER_OIDC_ISSUER_URL: "http://host.docker.internal:9000/application/o/coder/"
      CODER_OIDC_CLIENT_ID: "${CODER_OIDC_CLIENT_ID:-coder}"
      CODER_OIDC_CLIENT_SECRET: "${CODER_OIDC_CLIENT_SECRET:-}"
      CODER_OIDC_ALLOW_SIGNUPS: "true"
      CODER_OIDC_IGNORE_EMAIL_VERIFIED: "true"

      # OIDC Group-to-Role Mapping (Authentik → Coder RBAC)
      # Reads the "groups" claim from the OIDC token and maps Authentik groups to Coder roles.
      # Users in "coder-admins" get Owner, "coder-template-admins" get Template Admin,
      # "coder-auditors" get Auditor. Everyone else stays Member (default).
      # IMPORTANT: Groups must exist in Authentik AND be included in the OIDC scope.
      CODER_OIDC_GROUP_FIELD: "groups"
      CODER_OIDC_GROUP_MAPPING: |
        {
          "coder-admins": "owner",
          "coder-template-admins": "template-admin",
          "coder-auditors": "auditor"
        }
      # Sync Authentik group changes on every login (not just first login)
      CODER_OIDC_GROUP_AUTO_CREATE: "true"
      CODER_OIDC_USER_ROLE_FIELD: "groups"
      CODER_OIDC_USER_ROLE_MAPPING: |
        {
          "coder-admins": ["owner"],
          "coder-template-admins": ["template-admin"],
          "coder-auditors": ["auditor"]
        }

      # Disable default GitHub OAuth2 (shows by default in Coder 2.x)
      CODER_OAUTH2_GITHUB_DEFAULT_PROVIDER_ENABLE: "false"
    ports:
      - "7080:7080"
      - "7443:7443"
    volumes:
      # Mount Docker socket for Docker-based templates
      - /var/run/docker.sock:/var/run/docker.sock
      # Persistent Coder data
      - coder_data:/home/coder/.config/coderv2
      # TLS certificates for HTTPS
      - ./certs:/certs:ro
    networks:
      - coder-network
    # Docker socket access via group membership
    # Create coder user (UID 1000) and add to docker group
    # On host: sudo usermod -aG docker $USER (or set DOCKER_GID to docker group ID)
    group_add:
      - ${DOCKER_GID:-0}  # Default to root group (0) for Docker Desktop Mac compatibility
    # SECURITY: Run as non-root user (UID 1000)
    # Docker Desktop Mac: socket is owned by root:root, use DOCKER_GID=0
    # Linux: set DOCKER_GID to your docker group ID (usually 999 or 998)
    user: "1000:1000"

  # ==========================================================================
  # TEST DATABASE - PostgreSQL for connectivity testing
  # Internal network only - NOT exposed to host
  # Simulates internal company database that workspaces can access
  # ==========================================================================
  testdb:
    image: postgres:17-alpine
    container_name: testdb
    restart: unless-stopped
    environment:
      POSTGRES_USER: appuser
      POSTGRES_PASSWORD: ${TESTDB_PASSWORD:-testpassword}
      POSTGRES_DB: testapp
    volumes:
      - testdb_data:/var/lib/postgresql/data
      - ./testdb/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U appuser -d testapp"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - coder-network
    # NO ports mapping - internal network only

  # ==========================================================================
  # DEVDB - Developer Database Server
  # Provides individual and team databases for workspace development
  # Individual: dev_{username}  |  Team: team_{template_name}
  # ==========================================================================
  devdb:
    image: postgres:17-alpine
    container_name: devdb
    restart: unless-stopped
    environment:
      POSTGRES_USER: devdb_admin
      POSTGRES_PASSWORD: ${DEVDB_ADMIN_PASSWORD:-devdbadmin123}
      POSTGRES_DB: devdb
    volumes:
      - devdb_data:/var/lib/postgresql/data
      - ./devdb/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
      - ./devdb/pg_hba.conf:/etc/postgresql/pg_hba.conf:ro
    command: >
      postgres
      -c hba_file=/etc/postgresql/pg_hba.conf
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U devdb_admin -d devdb"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - coder-network
    # NO ports mapping - internal network only (accessible from workspaces)

  # ==========================================================================
  # PLATFORM ADMIN - Unified Administration Dashboard
  # Web UI for managing databases, services, and platform monitoring
  # ==========================================================================
  platform-admin:
    build:
      context: ./platform-admin
      dockerfile: Dockerfile
    container_name: platform-admin
    restart: unless-stopped
    depends_on:
      devdb:
        condition: service_healthy
      postgres:
        condition: service_healthy
      key-provisioner:
        condition: service_healthy
    environment:
      # Database connection (DevDB - provisioning data)
      DEVDB_HOST: devdb
      DEVDB_PORT: "5432"
      DEVDB_USER: devdb_admin
      DEVDB_PASSWORD: ${DEVDB_ADMIN_PASSWORD:-devdbadmin123}
      DEVDB_NAME: devdb
      # LiteLLM database (AI usage data)
      LITELLM_DB_HOST: postgres
      LITELLM_DB_PORT: "5432"
      LITELLM_DB_USER: litellm
      LITELLM_DB_PASSWORD: litellm
      LITELLM_DB_NAME: litellm
      # Coder API
      CODER_URL: http://coder-server:7080
      CODER_ADMIN_EMAIL: admin@example.com
      CODER_ADMIN_PASSWORD: ${CODER_ADMIN_PASSWORD:-CoderAdmin123!}
      # Service URLs for monitoring
      GITEA_URL: http://gitea:3000
      DRONE_URL: http://drone-server:80
      MINIO_URL: http://minio:9002
      AI_GATEWAY_URL: http://litellm:4000
      AUTHENTIK_URL: http://authentik-server:9000
      # LiteLLM admin API (for AI Keys management)
      LITELLM_URL: http://litellm:4000
      LITELLM_MASTER_KEY: ${LITELLM_MASTER_KEY:-}
      # Key Provisioner (for admin reset actions)
      KEY_PROVISIONER_URL: http://key-provisioner:8100
      PROVISIONER_SECRET: ${PROVISIONER_SECRET:-}
      # Admin credentials (local fallback when OIDC is unavailable)
      ADMIN_USERNAME: ${PLATFORM_ADMIN_USER:-admin}
      ADMIN_PASSWORD: ${PLATFORM_ADMIN_PASSWORD:-admin123}
      # OIDC via Authentik (created by setup-authentik-sso-full.sh)
      # Uses host.docker.internal so browser can reach Authentik's auth endpoint
      OIDC_ISSUER_URL: http://host.docker.internal:9000/application/o/platform-admin/
      OIDC_CLIENT_ID: ${PLATFORM_ADMIN_OIDC_CLIENT_ID:-platform-admin}
      OIDC_CLIENT_SECRET: ${PLATFORM_ADMIN_OIDC_CLIENT_SECRET:-}
    ports:
      - "5050:5000"
    networks:
      - coder-network
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://127.0.0.1:5000/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # ==========================================================================
  # GITEA - Self-hosted Git Server with native OIDC support
  # Provides Git repositories with access control for testing
  # ==========================================================================
  gitea:
    image: gitea/gitea:1.25.4
    container_name: gitea
    restart: unless-stopped
    environment:
      - USER_UID=1000
      - USER_GID=1000
      - GITEA__database__DB_TYPE=sqlite3
      - GITEA__database__PATH=/data/gitea/gitea.db
    ports:
      - "3000:3000"   # Web UI
      - "10022:22"    # SSH
    volumes:
      - gitea_data:/data
      - ./gitea/app.ini:/data/gitea/conf/app.ini
    networks:
      - coder-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/healthz"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s

  # ==========================================================================
  # DRONE CI - Continuous Integration Server
  # Provides CI pipelines for Python apps
  # ==========================================================================
  drone-server:
    image: drone/drone:2
    container_name: drone-server
    restart: unless-stopped
    depends_on:
      gitea:
        condition: service_healthy
    environment:
      - DRONE_GITEA_SERVER=http://host.docker.internal:3000
      - DRONE_GITEA_CLIENT_ID=${DRONE_GITEA_CLIENT_ID}
      - DRONE_GITEA_CLIENT_SECRET=${DRONE_GITEA_CLIENT_SECRET}
      - DRONE_RPC_SECRET=${DRONE_RPC_SECRET:-super-secret-rpc-key}
      - DRONE_SERVER_HOST=${DRONE_SERVER_HOST:-localhost:8080}
      - DRONE_SERVER_PROTO=http
      - DRONE_DATABASE_DRIVER=sqlite3
      - DRONE_DATABASE_DATASOURCE=/data/database.sqlite
      - DRONE_GIT_ALWAYS_AUTH=false
      - DRONE_USER_CREATE=username:gitea,admin:true
    ports:
      - "8080:80"
    volumes:
      - drone_data:/data
    networks:
      - coder-network

  drone-runner:
    image: drone/drone-runner-docker:1
    container_name: drone-runner
    restart: unless-stopped
    depends_on:
      - drone-server
    environment:
      - DRONE_RPC_PROTO=http
      - DRONE_RPC_HOST=drone-server
      - DRONE_RPC_SECRET=${DRONE_RPC_SECRET:-super-secret-rpc-key}
      - DRONE_RUNNER_CAPACITY=2
      - DRONE_RUNNER_NAME=docker-runner
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - coder-network

  # ==========================================================================
  # AI GATEWAY - DEPRECATED: Replaced by LiteLLM proxy (see litellm service)
  # ==========================================================================
  # ai-gateway:
  #   build:
  #     context: ./ai-gateway
  #     dockerfile: Dockerfile
  #   container_name: ai-gateway
  #   restart: unless-stopped
  #   depends_on:
  #     devdb:
  #       condition: service_healthy
  #   environment:
  #     # Anthropic Claude API
  #     - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
  #
  #     # AWS Bedrock
  #     - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID:-}
  #     - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY:-}
  #     - AWS_REGION=${AWS_REGION:-us-east-1}
  #
  #     # Google Gemini (planned)
  #     - GOOGLE_API_KEY=${GOOGLE_API_KEY:-}
  #
  #     # Gateway settings
  #     - AI_GATEWAY_PORT=8090
  #     - RATE_LIMIT_RPM=${AI_RATE_LIMIT_RPM:-60}
  #     - CONFIG_PATH=/app/config.yaml
  #
  #     # SECURITY: Authentication settings
  #     # Set AI_GATEWAY_AUTH_ENABLED=false only for local development
  #     - AI_GATEWAY_AUTH_ENABLED=${AI_GATEWAY_AUTH_ENABLED:-true}
  #     - AI_GATEWAY_AUTH_SECRET=${AI_GATEWAY_AUTH_SECRET:-}
  #     - AI_GATEWAY_ALLOWED_ORIGINS=${AI_GATEWAY_ALLOWED_ORIGINS:-http://localhost:7080,http://host.docker.internal:7080}
  #     - CODER_URL=http://coder-server:7080
  #
  #     # DevDB connection for usage tracking
  #     - DEVDB_HOST=devdb
  #     - DEVDB_PORT=5432
  #     - DEVDB_USER=ai_gateway
  #     - DEVDB_PASSWORD=${DEVDB_AI_GATEWAY_PASSWORD:-aigateway123}
  #     - DEVDB_NAME=devdb
  #   ports:
  #     - "8090:8090"
  #   volumes:
  #     - ./ai-gateway/config.yaml:/app/config.yaml:ro
  #     - ai_gateway_logs:/var/log/ai-gateway
  #   networks:
  #     - coder-network
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8090/health"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3

  # ==========================================================================
  # LITELLM PROXY - Centralized AI gateway with virtual keys and budgets
  # OpenAI-compatible API with per-user keys, budgets, rate limiting
  # Admin UI: http://localhost:4000/ui  API: http://localhost:4000
  # ==========================================================================
  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: litellm
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      langfuse-web:
        condition: service_healthy
    environment:
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID:-}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY:-}
      - AWS_REGION=${AWS_REGION:-us-east-1}
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY:-sk-poc-litellm-master-key-change-in-production}
      - DATABASE_URL=${LITELLM_DATABASE_URL:-postgresql://litellm:litellm@postgres:5432/litellm}
      - DEFAULT_ENFORCEMENT_LEVEL=${DEFAULT_ENFORCEMENT_LEVEL:-standard}
      # Content guardrails — PII/financial/secret detection
      - GUARDRAILS_ENABLED=${GUARDRAILS_ENABLED:-true}
      - DEFAULT_GUARDRAIL_LEVEL=${DEFAULT_GUARDRAIL_LEVEL:-standard}
      # Langfuse observability — async trace callback
      - LANGFUSE_PUBLIC_KEY=${LANGFUSE_PUBLIC_KEY:-lf_pk_poc_changeme}
      - LANGFUSE_SECRET_KEY=${LANGFUSE_SECRET_KEY:-lf_sk_poc_changeme}
      - LANGFUSE_HOST=http://langfuse-web:3000
      # OIDC SSO for LiteLLM Admin UI (via Authentik)
      - PROXY_BASE_URL=http://localhost:4000
      - GENERIC_CLIENT_ID=${LITELLM_OIDC_CLIENT_ID:-litellm}
      - GENERIC_CLIENT_SECRET=${LITELLM_OIDC_CLIENT_SECRET:-}
      - GENERIC_AUTHORIZATION_ENDPOINT=http://host.docker.internal:9000/application/o/authorize/
      - GENERIC_TOKEN_ENDPOINT=http://host.docker.internal:9000/application/o/token/
      - GENERIC_USERINFO_ENDPOINT=http://host.docker.internal:9000/application/o/userinfo/
      - GENERIC_SCOPE=openid profile email
    ports:
      - "4000:4000"
    volumes:
      - ./litellm/config.yaml:/app/config.yaml:ro
      - ../shared/litellm-hooks/enforcement_hook.py:/app/enforcement_hook.py:ro
      - ../shared/litellm-hooks/guardrails_hook.py:/app/guardrails_hook.py:ro
      - ../shared/litellm-hooks/prompts:/app/prompts:ro
      - ../shared/litellm-hooks/guardrails:/app/guardrails:ro
    command: ["--config", "/app/config.yaml", "--port", "4000"]
    networks:
      - coder-network
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:4000/health/readiness')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # ==========================================================================
  # KEY PROVISIONER - Scoped LiteLLM key management microservice
  # Isolates LiteLLM master key from workspace containers
  # Workspaces call this to auto-provision virtual keys
  # ==========================================================================
  key-provisioner:
    build:
      context: ../shared/key-provisioner
      dockerfile: Dockerfile
    container_name: key-provisioner
    restart: unless-stopped
    depends_on:
      litellm:
        condition: service_started
    environment:
      - LITELLM_URL=http://litellm:4000
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY:-sk-poc-litellm-master-key-change-in-production}
      - PROVISIONER_SECRET=${PROVISIONER_SECRET:-}
      - CODER_URL=http://coder-server:7080
      # Per-scope rate limits (overridable via .env)
      - WORKSPACE_RPM=${WORKSPACE_RPM:-60}
      - WORKSPACE_TPM=${WORKSPACE_TPM:-100000}
      - WORKSPACE_BUDGET=${WORKSPACE_BUDGET:-10.00}
      - WORKSPACE_BUDGET_DURATION=${WORKSPACE_BUDGET_DURATION:-1d}
      - WORKSPACE_MAX_PARALLEL=${WORKSPACE_MAX_PARALLEL:-5}
      - USER_RPM=${USER_RPM:-100}
      - USER_TPM=${USER_TPM:-200000}
      - USER_BUDGET=${USER_BUDGET:-20.00}
      - USER_BUDGET_DURATION=${USER_BUDGET_DURATION:-1d}
      - USER_MAX_PARALLEL=${USER_MAX_PARALLEL:-10}
      - CI_RPM=${CI_RPM:-30}
      - CI_TPM=${CI_TPM:-50000}
      - CI_BUDGET=${CI_BUDGET:-5.00}
      - CI_BUDGET_DURATION=${CI_BUDGET_DURATION:-1d}
      - CI_MAX_PARALLEL=${CI_MAX_PARALLEL:-3}
    ports:
      - "8100:8100"
    networks:
      - coder-network
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8100/health')"]
      interval: 5s
      timeout: 3s
      start_period: 3s
      retries: 2

  # ==========================================================================
  # LANGFUSE - AI Observability & Trace Analytics
  # Self-hosted LLM trace visualization, latency analytics, cost tracking
  # Receives traces from LiteLLM via async callback (NOT in request path)
  # Web UI: http://localhost:3100  API: http://localhost:3100/api/public
  # ==========================================================================

  # ClickHouse — columnar analytics DB for Langfuse trace queries
  clickhouse:
    image: clickhouse/clickhouse-server:latest
    container_name: clickhouse
    restart: unless-stopped
    environment:
      CLICKHOUSE_DB: langfuse
      CLICKHOUSE_USER: langfuse
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD:-clickhouse}
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - clickhouse_logs:/var/log/clickhouse-server
    networks:
      - coder-network
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8123/ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    # No host ports — internal only (Langfuse accesses via clickhouse:8123/9000)

  # MinIO bucket initializer for Langfuse blob storage
  mc-init:
    image: minio/mc:latest
    container_name: mc-init
    restart: "no"
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      mc alias set myminio http://minio:9002 $${MINIO_ROOT_USER} $${MINIO_ROOT_PASSWORD} &&
      mc mb --ignore-existing myminio/langfuse-events &&
      mc mb --ignore-existing myminio/langfuse-media &&
      echo 'Langfuse buckets created successfully'
      "
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minioadmin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-minioadmin}
    networks:
      - coder-network

  # Langfuse Web — UI + API server
  langfuse-web:
    image: docker.io/langfuse/langfuse:3
    container_name: langfuse-web
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
      minio:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      # Next.js 15 binds to container hostname by default; force all-interfaces
      HOSTNAME: "0.0.0.0"
      # Database
      DATABASE_URL: postgresql://langfuse:langfuse@postgres:5432/langfuse
      # ClickHouse
      CLICKHOUSE_URL: http://clickhouse:8123
      CLICKHOUSE_MIGRATION_URL: clickhouse://clickhouse:9000
      CLICKHOUSE_USER: langfuse
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD:-clickhouse}
      CLICKHOUSE_CLUSTER_ENABLED: "false"
      # Redis
      REDIS_CONNECTION_STRING: redis://redis:6379/1
      # S3/MinIO blob storage
      LANGFUSE_S3_EVENT_UPLOAD_BUCKET: langfuse-events
      LANGFUSE_S3_EVENT_UPLOAD_REGION: us-east-1
      LANGFUSE_S3_EVENT_UPLOAD_ENDPOINT: http://minio:9002
      LANGFUSE_S3_EVENT_UPLOAD_ACCESS_KEY_ID: ${MINIO_ROOT_USER:-minioadmin}
      LANGFUSE_S3_EVENT_UPLOAD_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD:-minioadmin}
      LANGFUSE_S3_EVENT_UPLOAD_FORCE_PATH_STYLE: "true"
      LANGFUSE_S3_MEDIA_UPLOAD_BUCKET: langfuse-media
      LANGFUSE_S3_MEDIA_UPLOAD_REGION: us-east-1
      LANGFUSE_S3_MEDIA_UPLOAD_ENDPOINT: http://minio:9002
      LANGFUSE_S3_MEDIA_UPLOAD_ACCESS_KEY_ID: ${MINIO_ROOT_USER:-minioadmin}
      LANGFUSE_S3_MEDIA_UPLOAD_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD:-minioadmin}
      LANGFUSE_S3_MEDIA_UPLOAD_FORCE_PATH_STYLE: "true"
      # Auth
      NEXTAUTH_URL: http://localhost:3100
      NEXTAUTH_SECRET: ${LANGFUSE_NEXTAUTH_SECRET:-change-me-nextauth-secret}
      SALT: ${LANGFUSE_SALT:-change-me-salt}
      ENCRYPTION_KEY: ${LANGFUSE_ENCRYPTION_KEY:-0000000000000000000000000000000000000000000000000000000000000000}
      # Headless init — auto-creates org, project, admin user, and API keys on first boot
      LANGFUSE_INIT_ORG_ID: poc-org
      LANGFUSE_INIT_ORG_NAME: "PoC Organization"
      LANGFUSE_INIT_PROJECT_ID: poc-project
      LANGFUSE_INIT_PROJECT_NAME: "AI Gateway Traces"
      LANGFUSE_INIT_PROJECT_PUBLIC_KEY: ${LANGFUSE_PUBLIC_KEY:-lf_pk_poc_changeme}
      LANGFUSE_INIT_PROJECT_SECRET_KEY: ${LANGFUSE_SECRET_KEY:-lf_sk_poc_changeme}
      LANGFUSE_INIT_USER_EMAIL: admin@local.test
      LANGFUSE_INIT_USER_NAME: admin
      LANGFUSE_INIT_USER_PASSWORD: ${LANGFUSE_ADMIN_PASSWORD:-adminadmin}
      # Telemetry
      TELEMETRY_ENABLED: "false"
      NEXT_TELEMETRY_DISABLED: "1"
    ports:
      - "3100:3000"
    networks:
      - coder-network
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://127.0.0.1:3000/api/public/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # Langfuse Worker — background job processor (trace ingestion, analytics)
  langfuse-worker:
    image: docker.io/langfuse/langfuse-worker:3
    container_name: langfuse-worker
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
      minio:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      # Database
      DATABASE_URL: postgresql://langfuse:langfuse@postgres:5432/langfuse
      # ClickHouse
      CLICKHOUSE_URL: http://clickhouse:8123
      CLICKHOUSE_MIGRATION_URL: clickhouse://clickhouse:9000
      CLICKHOUSE_USER: langfuse
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD:-clickhouse}
      CLICKHOUSE_CLUSTER_ENABLED: "false"
      # Redis
      REDIS_CONNECTION_STRING: redis://redis:6379/1
      # S3/MinIO blob storage
      LANGFUSE_S3_EVENT_UPLOAD_BUCKET: langfuse-events
      LANGFUSE_S3_EVENT_UPLOAD_REGION: us-east-1
      LANGFUSE_S3_EVENT_UPLOAD_ENDPOINT: http://minio:9002
      LANGFUSE_S3_EVENT_UPLOAD_ACCESS_KEY_ID: ${MINIO_ROOT_USER:-minioadmin}
      LANGFUSE_S3_EVENT_UPLOAD_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD:-minioadmin}
      LANGFUSE_S3_EVENT_UPLOAD_FORCE_PATH_STYLE: "true"
      LANGFUSE_S3_MEDIA_UPLOAD_BUCKET: langfuse-media
      LANGFUSE_S3_MEDIA_UPLOAD_REGION: us-east-1
      LANGFUSE_S3_MEDIA_UPLOAD_ENDPOINT: http://minio:9002
      LANGFUSE_S3_MEDIA_UPLOAD_ACCESS_KEY_ID: ${MINIO_ROOT_USER:-minioadmin}
      LANGFUSE_S3_MEDIA_UPLOAD_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD:-minioadmin}
      LANGFUSE_S3_MEDIA_UPLOAD_FORCE_PATH_STYLE: "true"
      # Auth (worker needs encryption key for data access)
      ENCRYPTION_KEY: ${LANGFUSE_ENCRYPTION_KEY:-0000000000000000000000000000000000000000000000000000000000000000}
      SALT: ${LANGFUSE_SALT:-change-me-salt}
      # Telemetry
      TELEMETRY_ENABLED: "false"
    networks:
      - coder-network

  # ==========================================================================
  # AUTHENTIK - Identity Provider & RBAC
  # Provides SSO, RBAC, approval workflows for contractor access
  # Optional Azure AD federation supported
  # Uses shared PostgreSQL with separate 'authentik' database
  # ==========================================================================
  redis:
    image: redis:7-alpine
    container_name: redis
    restart: unless-stopped
    command: --save 60 1 --loglevel warning
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - coder-network

  authentik-server:
    image: ghcr.io/goauthentik/server:2025.12
    container_name: authentik-server
    restart: unless-stopped
    command: server
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      AUTHENTIK_SECRET_KEY: ${AUTHENTIK_SECRET_KEY:-change-me-to-a-long-random-string}
      AUTHENTIK_POSTGRESQL__HOST: postgres
      AUTHENTIK_POSTGRESQL__USER: authentik
      AUTHENTIK_POSTGRESQL__PASSWORD: authentik
      AUTHENTIK_POSTGRESQL__NAME: authentik
      # Bootstrap admin user
      AUTHENTIK_BOOTSTRAP_PASSWORD: ${AUTHENTIK_ADMIN_PASSWORD:-admin}
      AUTHENTIK_BOOTSTRAP_EMAIL: admin@localhost
      # Optional Azure AD Federation (uncomment to enable)
      # AUTHENTIK_SOURCES__AZURE__CLIENT_ID: ${AZURE_CLIENT_ID:-}
      # AUTHENTIK_SOURCES__AZURE__CLIENT_SECRET: ${AZURE_CLIENT_SECRET:-}
    ports:
      - "9000:9000"      # HTTP
      - "9443:9443"      # HTTPS
    volumes:
      - authentik_media:/media
      - authentik_templates:/templates
    networks:
      - coder-network
    healthcheck:
      test: ["CMD-SHELL", "python -c \"import urllib.request; urllib.request.urlopen('http://localhost:9000/-/health/ready/')\""]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  authentik-worker:
    image: ghcr.io/goauthentik/server:2025.12
    container_name: authentik-worker
    restart: unless-stopped
    command: worker
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      AUTHENTIK_SECRET_KEY: ${AUTHENTIK_SECRET_KEY:-change-me-to-a-long-random-string}
      AUTHENTIK_POSTGRESQL__HOST: postgres
      AUTHENTIK_POSTGRESQL__USER: authentik
      AUTHENTIK_POSTGRESQL__PASSWORD: authentik
      AUTHENTIK_POSTGRESQL__NAME: authentik
    volumes:
      - authentik_media:/media
      - authentik_templates:/templates
    networks:
      - coder-network

  # ==========================================================================
  # MINIO - S3-Compatible Object Storage
  # Provides artifact storage, build outputs, file sharing between workspaces
  # Console: http://localhost:9001  API: http://localhost:9002
  # ==========================================================================
  minio:
    image: minio/minio:latest
    container_name: minio
    restart: unless-stopped
    command: server /data --console-address ":9001" --address ":9002"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minioadmin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-minioadmin}
      # OIDC SSO via Authentik
      MINIO_IDENTITY_OPENID_CONFIG_URL: http://host.docker.internal:9000/application/o/minio/.well-known/openid-configuration
      MINIO_IDENTITY_OPENID_CLIENT_ID: ${MINIO_IDENTITY_OPENID_CLIENT_ID:-minio}
      MINIO_IDENTITY_OPENID_CLIENT_SECRET: ${MINIO_IDENTITY_OPENID_CLIENT_SECRET:-}
      MINIO_IDENTITY_OPENID_CLAIM_NAME: policy
      MINIO_IDENTITY_OPENID_SCOPES: "openid,profile,email,minio"
      MINIO_IDENTITY_OPENID_REDIRECT_URI: http://localhost:9001/oauth_callback
      MINIO_IDENTITY_OPENID_DISPLAY_NAME: "Login with Authentik"
    ports:
      - "9001:9001"   # Console UI
      - "9002:9002"   # S3 API
    volumes:
      - minio_data:/data
    networks:
      - coder-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9002/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ==========================================================================
  # MAILPIT - Email Testing Server
  # Captures all outgoing emails for testing/development
  # Web UI: http://localhost:8025  SMTP: mailpit:1025
  # ==========================================================================
  mailpit:
    image: axllent/mailpit:latest
    container_name: mailpit
    restart: unless-stopped
    ports:
      - "8025:8025"   # Web UI
      - "1025:1025"   # SMTP
    environment:
      MP_SMTP_AUTH_ACCEPT_ANY: 1
      MP_SMTP_AUTH_ALLOW_INSECURE: 1
    networks:
      - coder-network
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8025/"]
      interval: 10s
      timeout: 5s
      retries: 3

  # ==========================================================================
  # PORTAL - Demo Landing Page
  # Static HTML page with links to all services, credentials, and quick start
  # Web UI: http://localhost:3333
  # ==========================================================================
  portal:
    image: nginx:alpine
    container_name: portal
    restart: unless-stopped
    ports:
      - "3333:80"
    volumes:
      - ./portal/index.html:/usr/share/nginx/html/index.html:ro
      - ./portal/nginx.conf:/etc/nginx/conf.d/default.conf:ro
    networks:
      - coder-network

  # Optional: Grafana for monitoring (uncomment to enable)
  # grafana:
  #   image: grafana/grafana:latest
  #   container_name: coder-grafana
  #   restart: unless-stopped
  #   environment:
  #     GF_AUTH_ANONYMOUS_ENABLED: "true"
  #     GF_AUTH_ANONYMOUS_ORG_ROLE: "Viewer"
  #   ports:
  #     - "3000:3000"
  #   volumes:
  #     - grafana_data:/var/lib/grafana
  #   networks:
  #     - coder-network

volumes:
  minio_data:
    name: coder-poc-minio
  postgres_data:
    name: coder-poc-postgres
  testdb_data:
    name: coder-poc-testdb
  devdb_data:
    name: coder-poc-devdb
  coder_data:
    name: coder-poc-data
  gitea_data:
    name: coder-poc-gitea
  drone_data:
    name: coder-poc-drone
  # ai_gateway_logs:
  #   name: coder-poc-ai-gateway-logs
  clickhouse_data:
    name: coder-poc-clickhouse
  clickhouse_logs:
    name: coder-poc-clickhouse-logs
  redis_data:
    name: coder-poc-redis
  authentik_media:
    name: coder-poc-authentik-media
  authentik_templates:
    name: coder-poc-authentik-templates
  # grafana_data:
  #   name: coder-poc-grafana

networks:
  coder-network:
    name: coder-network
    driver: bridge
